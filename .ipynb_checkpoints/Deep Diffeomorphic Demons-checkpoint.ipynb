{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37355664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_25748/3383047370.py\", line 14, in <module>\n",
      "    import transformers, utils, metrics\n",
      "ModuleNotFoundError: No module named 'utils'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_25748/3383047370.py\", line 14, in <module>\n",
      "    import transformers, utils, metrics\n",
      "ModuleNotFoundError: No module named 'utils'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_25748/3383047370.py\", line 14, in <module>\n",
      "    import transformers, utils, metrics\n",
      "ModuleNotFoundError: No module named 'utils'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3173, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3383, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Demons registration with variants of demons forces, as well as the diffeomorphic demons.\n",
    "\n",
    "@author: Xinzhe Luo\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_probability as tfp\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "import transformers as transformer\n",
    "import python_utils as utils\n",
    "import metrics\n",
    "import logging\n",
    "import shutil\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "def _load_data(filename, dtype=np.float32, **kwargs):\n",
    "    \"\"\"\n",
    "    Load from nifty files.\n",
    "\n",
    "    :param filename: The file name to extract data.\n",
    "    :param dtype: The output data type.\n",
    "    :param num_channels: Number of channels.\n",
    "    :param normalization: Whether to normalize the data using min-max normalization.\n",
    "    :param clip_value: whether to clip value by the 99 percentile\n",
    "    :return: An array.\n",
    "    \"\"\"\n",
    "    num_channels = kwargs.pop('num_channels', 1)\n",
    "    normalization = kwargs.pop('normalization', None)\n",
    "    clip_value = kwargs.pop('clip_value', True)\n",
    "    data_format = kwargs.pop('data_format', 'nifty')\n",
    "    assert data_format in ['nifty', 'png'], \"Data format must be 'nifty' or 'png'!\"\n",
    "    assert normalization in [None, 'min-max', 'z-score']\n",
    "\n",
    "    if data_format == 'nifty':\n",
    "        img = nib.load(filename)\n",
    "        data = np.asarray(img.get_fdata(), dtype=dtype)\n",
    "    elif data_format == 'png':\n",
    "        img = cv2.imread(filename)\n",
    "        data = np.asarray(img[..., 0], dtype=dtype)\n",
    "        image_size = kwargs.pop('image_size', None)\n",
    "        if image_size is not None:\n",
    "            data = data[:image_size[0], :image_size[1]]\n",
    "        data[data != 255] = 0\n",
    "\n",
    "    if len(data.shape) == 2:\n",
    "        data = np.tile(np.expand_dims(data, -1), (1, 1, 5))\n",
    "\n",
    "    if clip_value:\n",
    "        data = np.clip(data, -np.inf, np.percentile(data, 99))\n",
    "\n",
    "    if normalization:\n",
    "        if normalization == 'min-max':\n",
    "            # min-max normalization\n",
    "            data_loc = data - np.min(data)\n",
    "            data_norm = data_loc / np.max(data_loc)\n",
    "\n",
    "        elif normalization == 'z-score':\n",
    "            # z-score normalization\n",
    "            data_norm = stats.zscore(data, axis=None, ddof=1)\n",
    "\n",
    "        data_expand = np.expand_dims(data_norm, axis=-1)\n",
    "    else:\n",
    "        data_expand = np.expand_dims(data, axis=-1)\n",
    "\n",
    "    if data_format == 'nifty':\n",
    "        return np.expand_dims(np.tile(data_expand, np.hstack((np.ones(data.ndim), num_channels))), 0), img.affine, \\\n",
    "               img.header\n",
    "    elif data_format == 'png':\n",
    "        return np.expand_dims(np.tile(data_expand, np.hstack((np.ones(data.ndim), num_channels))), 0)\n",
    "\n",
    "\n",
    "def _one_hot_label(data, labels=(0, 1)):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param data: of shape [1, *vol_shape, 1]\n",
    "    :param labels: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    data = data.squeeze((0, -1))\n",
    "    n_class = len(labels)\n",
    "    label = np.zeros((np.hstack((data.shape, n_class))), dtype=np.float32)\n",
    "\n",
    "    for k in range(1, n_class):\n",
    "        label[..., k] = (data == labels[k])\n",
    "\n",
    "    label[..., 0] = np.logical_not(np.sum(label[..., 1:], axis=-1))\n",
    "\n",
    "    return np.expand_dims(label, 0)\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    A unified class to perform various types of demons registration.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=(96, 96, 96), num_channels=1, num_classes=2,\n",
    "                 demons_type='compositive', demons_force='moving', **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the trainer.\n",
    "\n",
    "        :param input_size: The input size.\n",
    "        :param demons_type: The demons type, 'compositive', 'additive' or 'diffeomorphic'.\n",
    "        :param demons_force: The demons force, 'moving', 'fixed' or 'symmetric'.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        assert demons_type in ['compositive', 'additive', 'diffeomorphic']\n",
    "        assert demons_force in ['moving', 'fixed', 'symmetric']\n",
    "        self.input_size = input_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.demons_type = demons_type\n",
    "        self.demons_force = demons_force\n",
    "        self.kwargs = kwargs\n",
    "        self.regularizer = self.kwargs.pop('regularizer', 'fluid')\n",
    "        assert self.regularizer in ['fluid', 'diffusion']\n",
    "        self.normalization = self.kwargs.pop('normalization', None)\n",
    "        self.max_length = self.kwargs.pop('max_length', 2.)\n",
    "        self.exp_steps = self.kwargs.pop('exp_steps', 4) if demons_type == 'diffeomorphic' else 0\n",
    "        self.logger = kwargs.get(\"logger\", logging)\n",
    "\n",
    "        # initialize placeholder\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.target_image = tf.placeholder(tf.float32, [1, input_size[0], input_size[1],\n",
    "                                                            input_size[2], num_channels], name='target_image')\n",
    "            self.target_label = tf.placeholder(tf.float32, [1, input_size[0], input_size[1],\n",
    "                                                            input_size[2], num_classes], name='target_label')\n",
    "            self.source_image = tf.placeholder(tf.float32, [1, input_size[0], input_size[1],\n",
    "                                                            input_size[2], num_channels], name='source_image')\n",
    "            self.source_label = tf.placeholder(tf.float32, [1, input_size[0], input_size[1],\n",
    "                                                            input_size[2], num_classes], name='source_label')\n",
    "\n",
    "        # spatial transformation\n",
    "        with tf.variable_scope('spatial_transformation'):\n",
    "            # create the vector fields\n",
    "            self.vector_fields = tf.get_variable('vector_fields', shape=[1, input_size[0], input_size[1],\n",
    "                                                                         input_size[2], 3],\n",
    "                                                 initializer=tf.zeros_initializer(), trainable=True)\n",
    "\n",
    "            self.warped_source_image = transformer.SpatialTransformer(interp_method='linear',\n",
    "                                                                      name='warp_source_image')([self.source_image,\n",
    "                                                                                                 self.vector_fields])\n",
    "            self.warped_source_label = transformer.SpatialTransformer(interp_method='nearest',\n",
    "                                                                      name='warp_source_label')([self.source_label,\n",
    "                                                                                                 self.vector_fields])\n",
    "\n",
    "        # get train_op\n",
    "        self._train_op = self._get_optimizer()\n",
    "\n",
    "        # metrics\n",
    "        with tf.name_scope('metrics'):\n",
    "            self.mse = tf.reduce_mean(tf.square(self.target_image - self.warped_source_image), name='mse')\n",
    "            self.dice = metrics.OverlapMetrics(num_classes).averaged_foreground_dice(y_true=self.target_label,\n",
    "                                                                                     y_seg=self.warped_source_label)\n",
    "            self.bending_energy = metrics.local_displacement_energy(self.vector_fields, energy_type='bending')\n",
    "            self.jacobian_det = metrics.compute_jacobian_determinant(self.vector_fields)\n",
    "            self.num_neg_jacob = tf.math.count_nonzero(tf.less_equal(self.jacobian_det, 0), dtype=tf.float32,\n",
    "                                                       name='negative_jacobians_number')\n",
    "            self.mean_jacobian = tf.reduce_mean(self.jacobian_det, name='mean_jacobian_det')\n",
    "            self.min_jacobian = tf.reduce_min(self.jacobian_det, name='min_jacobian_det')\n",
    "            self.max_jacobian = tf.reduce_max(self.jacobian_det, name='max_jacobian_det')\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        \"\"\"\n",
    "        Get optimizer for registration optimization.\n",
    "\n",
    "        :return: The train-operation.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('optimizer'):\n",
    "\n",
    "            # image normalization\n",
    "            with tf.name_scope('image_normalization'):\n",
    "                target_norm_image = utils.normalize_image(self.target_image, self.normalization)\n",
    "                source_norm_image = utils.normalize_image(self.source_image, self.normalization)\n",
    "\n",
    "            # compute the update fields\n",
    "            with tf.name_scope('get_update_fields'):\n",
    "                warped_source_norm_image = transformer.SpatialTransformer()([source_norm_image, self.vector_fields])\n",
    "\n",
    "                # compute intensity difference\n",
    "                diff = tf.subtract(target_norm_image, warped_source_norm_image, name='intensity_diff')\n",
    "\n",
    "                # compute demons forces\n",
    "                if self.demons_force == 'fixed':\n",
    "                    jacobian = - utils.compute_image_gradient(target_norm_image)\n",
    "                elif self.demons_force == 'moving':\n",
    "                    jacobian = - utils.compute_image_gradient(warped_source_norm_image)\n",
    "                elif self.demons_force == 'symmetric':\n",
    "                    jacobian = - (utils.compute_image_gradient(target_norm_image) + utils.compute_image_gradient(\n",
    "                        warped_source_norm_image)) / 2\n",
    "\n",
    "                # get correspondence update fields\n",
    "                update_fields = tf.negative((diff * jacobian) / (tf.norm(jacobian, axis=-1,\n",
    "                                                                         keepdims=True) ** 2 + diff ** 2),\n",
    "                                            name='update_fields')\n",
    "\n",
    "                # control update fields by the maximum step length\n",
    "                update_fields /= tf.reduce_max(tf.norm(update_fields, axis=-1))\n",
    "                update_fields *= self.max_length\n",
    "\n",
    "            # fluid-like regularization\n",
    "            if self.regularizer == 'fluid':\n",
    "                with tf.name_scope('fluid_regularization'):\n",
    "                    update_fields = utils.separable_gaussian_filter3d(update_fields, utils.gauss_kernel1d(sigma=1.))\n",
    "\n",
    "            # update the correspondence fields\n",
    "            with tf.name_scope('update_correspondence_fields'):\n",
    "                if self.demons_type == 'compositive':\n",
    "                    corres_fields = update_fields + transformer.SpatialTransformer(\n",
    "                        interp_method='linear', name='warp_vector_fields')([self.vector_fields,\n",
    "                                                                            update_fields])\n",
    "\n",
    "                elif self.demons_type == 'additive':\n",
    "                    corres_fields = self.vector_fields + update_fields\n",
    "\n",
    "                elif self.demons_type == 'diffeomorphic':\n",
    "                    exp_update_fields = tf.expand_dims(transformer.integrate_vec(tf.squeeze(update_fields, 0),\n",
    "                                                                                 method='ss', nb_steps=self.exp_steps),\n",
    "                                                       axis=0, name='exp_update_fields')\n",
    "                    corres_fields = exp_update_fields + transformer.SpatialTransformer(\n",
    "                        interp_method='linear', name='warp_fields')([self.vector_fields, exp_update_fields])\n",
    "\n",
    "            # diffusion-like regularization\n",
    "            if self.regularizer == 'diffusion':\n",
    "                with tf.name_scope('diffusion_regularization'):\n",
    "                    corres_fields = utils.separable_gaussian_filter3d(corres_fields, utils.gauss_kernel1d(1.))\n",
    "\n",
    "            train_op = tf.assign(self.vector_fields, corres_fields, validate_shape=True, name='train_op')\n",
    "\n",
    "        return train_op\n",
    "\n",
    "    def _initialize(self, training_iters, model_path, restore, prediction_path):\n",
    "        self.training_iters = training_iters\n",
    "        self.model_path = model_path\n",
    "        self.prediction_path = prediction_path\n",
    "        abs_prediction_path = os.path.abspath(prediction_path)\n",
    "        abs_model_path = os.path.abspath(model_path)\n",
    "\n",
    "        # remove the previous directory for model storing and validation prediction\n",
    "        if not restore:\n",
    "            self.logger.info(\"Removing '{:}'\".format(abs_prediction_path))\n",
    "            shutil.rmtree(abs_prediction_path, ignore_errors=True)\n",
    "            self.logger.info(\"Removing '{:}'\".format(abs_model_path))\n",
    "            shutil.rmtree(abs_model_path, ignore_errors=True)\n",
    "\n",
    "        # create a new directory for model storing and validation prediction\n",
    "        if not os.path.exists(abs_prediction_path):\n",
    "            self.logger.info(\"Allocating '{:}'\".format(abs_prediction_path))\n",
    "            os.makedirs(abs_prediction_path)\n",
    "\n",
    "        if not os.path.exists(abs_model_path):\n",
    "            self.logger.info(\"Allocating '{:}'\".format(abs_model_path))\n",
    "            os.makedirs(abs_model_path)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        return init\n",
    "\n",
    "    def save(self, sess, model_path, latest_filename=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Saves the current session to a checkpoint\n",
    "\n",
    "        :param sess: current session\n",
    "        :param model_path: path to file system location\n",
    "        :param latest_filename: Optional name for the protocol buffer file that will contains the list of most recent\n",
    "        checkpoints.\n",
    "        \"\"\"\n",
    "\n",
    "        saver = tf.train.Saver(**kwargs)\n",
    "        save_path = saver.save(sess, model_path, latest_filename=latest_filename)\n",
    "        self.logger.info(\"Model saved to file: %s\" % save_path)\n",
    "        return save_path\n",
    "\n",
    "    def restore(self, sess, model_path, **kwargs):\n",
    "        \"\"\"\n",
    "        Restores a session from a checkpoint\n",
    "\n",
    "        :param sess: current session instance\n",
    "        :param model_path: path to file system checkpoint location\n",
    "        \"\"\"\n",
    "\n",
    "        saver = tf.train.Saver(**kwargs)\n",
    "        saver.restore(sess, model_path)\n",
    "        self.logger.info(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    def train(self, data, training_iters, display_steps=1, restore=False, model_path='model_trained',\n",
    "              prediction_path='prediction', **kwargs):\n",
    "        \"\"\"\n",
    "        Train model on the given data.\n",
    "\n",
    "        :param data: The data dictionary containing 'target_image', 'target_label', 'source_image', 'source_label',\n",
    "                     'target_header', 'target_affine'.\n",
    "        :param training_iters: number of training iterations\n",
    "        :param display_steps: number of training steps till displaying the metrics\n",
    "        :param restore: whether to restore the model\n",
    "        :param model_path: where to save/restore the model\n",
    "        :param prediction_path: where to store predictions\n",
    "        :return: metrics - the training metrics;\n",
    "                 save_path - the saved model path\n",
    "        \"\"\"\n",
    "\n",
    "        save_path = os.path.join(model_path, \"model.ckpt\")\n",
    "        init = self._initialize(training_iters, model_path, restore, prediction_path)\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            sess.run(init)\n",
    "\n",
    "            if restore:\n",
    "                ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    self.logger.info(\"Restoring previous model from path: %s\" % ckpt.model_checkpoint_path)\n",
    "                    self.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "            metrics = {\"MSE\": {}, \"Dice\": {}, \"Bending energy\": {}, \"Mean Jacobian determinant\": {},\n",
    "                       \"Min Jacobian determinant\": {}, \"Max Jacobian determinant\": {},\n",
    "                       \"# Negative Jacobians\": {}}\n",
    "\n",
    "            self.logger.info(\"Start optimization with demons configuration: demons force: {demons_force}, \"\n",
    "                             \"demons type: {demons_type}, maximum step length: {max_length:.1f} pixels, \"\n",
    "                             \"exponential steps: {exp_steps:.1f}\".format(demons_force=self.demons_force,\n",
    "                                                                         demons_type=self.demons_type,\n",
    "                                                                         max_length=self.max_length,\n",
    "                                                                         exp_steps=self.exp_steps)\n",
    "                             )\n",
    "\n",
    "            for step in range(training_iters):\n",
    "                _, mse, dice, bending_energy, \\\n",
    "                mean_jacobian, min_jacobian,\\\n",
    "                max_jacobian, num_neg_jacob = sess.run((self._train_op, self.mse, self.dice,\n",
    "                                                        self.bending_energy, self.mean_jacobian,\n",
    "                                                        self.min_jacobian, self.max_jacobian,\n",
    "                                                        self.num_neg_jacob),\n",
    "                                                       feed_dict={self.target_image: data['target_image'],\n",
    "                                                                  self.target_label: data['target_label'],\n",
    "                                                                  self.source_image: data['source_image'],\n",
    "                                                                  self.source_label: data['source_label']\n",
    "                                                                  }\n",
    "                                                       )\n",
    "\n",
    "                # record metrics\n",
    "                metrics['MSE'][step] = mse\n",
    "                metrics['Dice'][step] = dice\n",
    "                metrics['Bending energy'][step] = bending_energy\n",
    "                metrics['Mean Jacobian determinant'][step] = mean_jacobian\n",
    "                metrics['Max Jacobian determinant'][step] = max_jacobian\n",
    "                metrics['Min Jacobian determinant'][step] = min_jacobian\n",
    "                metrics['# Negative Jacobians'][step] = num_neg_jacob\n",
    "\n",
    "                self.logger.info(\"[Iteration {:}], [Metrics] MSE= {:.4f}, Dice= {:.4f}, Bending energy= {:.4f}, \"\n",
    "                                 \"Mean Jacobian det= {:.4f}, Max Jacobian det= {:.4f}, \"\n",
    "                                 \"Min Jacobian det= {:.4f}, \"\n",
    "                                 \"Negative Jacobians number= {:.4f}\".format(step, mse, dice, bending_energy,\n",
    "                                                                            mean_jacobian, max_jacobian,\n",
    "                                                                            min_jacobian, num_neg_jacob))\n",
    "\n",
    "                if step % display_steps == 0:\n",
    "                    self.store_prediction(sess, data, save_prefix='step%s_' % step, **kwargs)\n",
    "                    # utils.visualise_metrics([metrics],\n",
    "                    #                         save_path=self.prediction_path,\n",
    "                    #                         labels=[self.demons_type + '_demons_' + self.demons_force + '_force'])\n",
    "\n",
    "            self.logger.info(\"Optimization Finished!\")\n",
    "\n",
    "            # store final predictions\n",
    "            self.store_prediction(sess, data, save_prefix='final_', **kwargs)\n",
    "\n",
    "            self.logger.info(\"Final Predictions Stored!\")\n",
    "\n",
    "            # save model\n",
    "            self.save(sess, save_path)\n",
    "\n",
    "        return metrics, save_path\n",
    "\n",
    "    def store_prediction(self, sess, data, save_prefix, **kwargs):\n",
    "        warped_image, warped_label, jacobian_det, vector_fields = sess.run((self.warped_source_image,\n",
    "                                                                            self.warped_source_label,\n",
    "                                                                            self.jacobian_det,\n",
    "                                                                            self.vector_fields),\n",
    "                                                                           feed_dict={self.source_image: data['source_image'],\n",
    "                                                                                      self.source_label: data['source_label']\n",
    "                                                                                      }\n",
    "                                                                           )\n",
    "\n",
    "        # utils.save_prediction_png(data['target_image'], data['target_label'], warped_label,\n",
    "        #                           self.prediction_path, save_prefix=save_prefix)\n",
    "\n",
    "        utils.save_prediction_nii(jacobian_det.squeeze(0), self.prediction_path, data_type='jacobian',\n",
    "                                  affine=data['target_affine'], header=data['target_header'],\n",
    "                                  save_prefix=save_prefix, save_name='jacobian_det')\n",
    "\n",
    "        utils.save_prediction_nii(warped_image.squeeze(0), self.prediction_path, data_type='image',\n",
    "                                  affine=data['target_affine'], header=data['target_header'],\n",
    "                                  save_prefix=save_prefix)\n",
    "\n",
    "        utils.save_prediction_nii(warped_label.squeeze(0), self.prediction_path, data_type='label',\n",
    "                                  affine=data['target_affine'], header=data['target_header'],\n",
    "                                  save_prefix=save_prefix, **kwargs)\n",
    "\n",
    "        utils.save_prediction_nii(vector_fields.squeeze(0), self.prediction_path, affine=data['target_affine'],\n",
    "                                  data_type='vector_fields', header=data['target_header'], save_prefix=save_prefix)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from datetime import datetime\n",
    "\n",
    "    t = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Diffeomorphic demons registration for 3D images')\n",
    "    parser.add_argument('--demons_type', default='additive', choices=['additive', 'compositive', 'diffeomorphic'],\n",
    "                        help='the demons type')\n",
    "    parser.add_argument('--demons_force', default='moving', choices=['moving', 'fixed', 'symmetric'],\n",
    "                        help='the demons force')\n",
    "    parser.add_argument('--regularizer', default='fluid', choices=['fluid', 'diffusion'],\n",
    "                        help='type of regularization for the displacement field')\n",
    "    parser.add_argument('--normalization', default='z-score', choices=[None, 'min-max', 'z-score'],\n",
    "                        help='type of image normalization')\n",
    "    parser.add_argument('--max_length', default=2., type=float, help='the maximum step length')\n",
    "    parser.add_argument('--exp_steps', default=7, type=int, help='the number of exponential steps')\n",
    "    parser.add_argument('--training_iters', default=30, type=int, help='number of training iterations')\n",
    "    parser.add_argument('--display_steps', default=5, type=int, help='number of iteration till displaying the result')\n",
    "    parser.add_argument('--cuda_device', default=-1, type=int, help='the GPU to deploy the model')\n",
    "    parser.add_argument('--config_name', default=t + '_config.txt', type=str,\n",
    "                        help='the filename to write down configurations')\n",
    "    parser.add_argument('--save_prefix', default=t + '_', type=str,\n",
    "                        help='the save path prefix for the trained model and predictions')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # set working directory\n",
    "    print(\"Working directory: %s\" % os.getcwd())\n",
    "    os.chdir('../')\n",
    "    print(\"Working directory changed to %s\" % os.getcwd())\n",
    "    \n",
    "    # set saving directory\n",
    "    model_path = args.save_prefix + '_model_trained'\n",
    "    prediction_path = args.save_prefix + '_prediction'\n",
    "\n",
    "    # prepare data\n",
    "    target_image, target_affine, target_header = _load_data('./data/crop_ADNI_I118671_image.nii.gz')\n",
    "    target_label = _one_hot_label(_load_data('./data/crop_ADNI_I118671_label.nii.gz', clip_value=False)[0],\n",
    "                                  labels=[0., 8., 9., 19., 20., 25., 26., 27., 28.])\n",
    "\n",
    "    source_image = _load_data('./data/affine_crop_ADNI_I118673_image.nii.gz')[0]\n",
    "    source_label = _one_hot_label(_load_data('./data/affine_crop_ADNI_I118673_label.nii.gz', clip_value=False)[0],\n",
    "                                  labels=[0., 8., 9., 19., 20., 25., 26., 27., 28.])\n",
    "\n",
    "    training_data = {'target_image': target_image, 'target_label': target_label,\n",
    "                     'source_image': source_image, 'source_label': source_label,\n",
    "                     'target_affine': target_affine, 'target_header': target_header}\n",
    "\n",
    "    # set device\n",
    "    device = '/cpu:0' if args.cuda_device == -1 else '/gpu:%s' % args.cuda_device\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)\n",
    "\n",
    "    with tf.Graph().as_default(), tf.device(device):\n",
    "        # trainer\n",
    "        trainer = Trainer(input_size=(149, 181, 159), num_channels=1, num_classes=9,\n",
    "                          demons_type=args.demons_type, demons_force=args.demons_force,\n",
    "                          normalization=args.normalization, regularizer=args.regularizer,\n",
    "                          max_length=args.max_length, exp_steps=args.exp_steps)\n",
    "\n",
    "        metrics, save_path = trainer.train(training_data, training_iters=args.training_iters, \n",
    "                                           display_steps=args.display_steps,\n",
    "                                           model_path=model_path,\n",
    "                                           prediction_path=prediction_path)\n",
    "\n",
    "    # write configurations\n",
    "    f = open(os.path.join(model_path, args.config_name), 'w+')\n",
    "    f.write(str(args))\n",
    "\n",
    "    # save metrics into excel files\n",
    "    metrics_df = pd.DataFrame(metrics, dtype=np.float32)\n",
    "    metrics_df.to_csv(os.path.join(prediction_path, 'metrics.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from trainer_3d import _load_data, _one_hot_label, Trainer\n",
    "\n",
    "target_path = './data/C.png'\n",
    "source_path = './data/circle.png'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # set working directory\n",
    "    print(\"Working directory: %s\" % os.getcwd())\n",
    "    os.chdir('../')\n",
    "    print(\"Working directory changed to %s\" % os.getcwd())\n",
    "\n",
    "    # set saving directory\n",
    "    model_path = 'circle2C_model_trained'\n",
    "    prediction_path = 'circle2C_prediction'\n",
    "\n",
    "    # load data\n",
    "    target_image = _load_data(target_path, data_format='png', image_size=(592, 592))\n",
    "    source_image = _load_data(source_path, data_format='png', image_size=(592, 592))\n",
    "\n",
    "    target_label = _one_hot_label(target_image, labels=(0, 255))\n",
    "    source_label = _one_hot_label(source_image, labels=(0, 255))\n",
    "\n",
    "    training_data = {'target_image': target_image, 'target_label': target_label,\n",
    "                     'source_image': source_image, 'source_label': source_label,\n",
    "                     'target_affine': np.eye(4), 'target_header': None\n",
    "                     }\n",
    "\n",
    "\n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        # trainer\n",
    "        trainer = Trainer(input_size=(592, 592, 5), num_channels=1, num_classes=2,\n",
    "                          demons_type='additive', demons_force='moving', normalization='z-score',\n",
    "                          max_length=2., exp_steps=7)\n",
    "\n",
    "        metrics, save_path = trainer.train(training_data, training_iters=500,\n",
    "                                           display_steps=5,\n",
    "                                           model_path=model_path,\n",
    "                                           prediction_path=prediction_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2-GPU",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
